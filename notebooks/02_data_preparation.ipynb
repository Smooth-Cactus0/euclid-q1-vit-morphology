{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 — Data Preparation: Pipeline Validation\n",
    "\n",
    "**Goal**: Validate the data pipeline end-to-end before training.\n",
    "\n",
    "This notebook:\n",
    "1. Downloads and extracts galaxy cutouts (or uses existing images)\n",
    "2. Loads the stratified splits\n",
    "3. Instantiates the `EuclidDataset` with transforms\n",
    "4. Verifies image loading, target shapes, and mask correctness\n",
    "5. Visualizes sample images with their morphology labels\n",
    "6. Tests a DataLoader batch for training readiness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path so we can import src/\n",
    "PROJECT_ROOT = Path('..').resolve()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.data.dataset import EuclidDataset, MorphologySchema\n",
    "from src.data.transforms import get_transforms, get_tta_transforms\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "\n",
    "CATALOG_PATH = PROJECT_ROOT / 'data' / 'raw' / 'morphology_catalogue.parquet'\n",
    "SPLIT_PATH = PROJECT_ROOT / 'data' / 'processed' / 'split_indices.json'\n",
    "IMAGE_DIR = PROJECT_ROOT / 'data' / 'raw' / 'images'\n",
    "FIGURES_DIR = PROJECT_ROOT / 'results' / 'figures'\n",
    "\n",
    "print(f'Project root: {PROJECT_ROOT}')\n",
    "print(f'Catalog exists: {CATALOG_PATH.exists()}')\n",
    "print(f'Splits exist: {SPLIT_PATH.exists()}')\n",
    "print(f'Images exist: {IMAGE_DIR.exists()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Verify Split Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SPLIT_PATH) as f:\n",
    "    split_data = json.load(f)\n",
    "\n",
    "meta = split_data['meta']\n",
    "print(f\"Total galaxies: {meta['total_galaxies']:,}\")\n",
    "print(f\"Seed: {meta['seed']}\")\n",
    "print(f\"Training cuts applied: {meta['apply_training_cuts']}\")\n",
    "print(f\"\\nSplit sizes:\")\n",
    "for split, size in meta['split_sizes'].items():\n",
    "    print(f\"  {split:6s}: {size:>8,} ({size/meta['total_galaxies']*100:.1f}%)\")\n",
    "\n",
    "# Verify no overlap\n",
    "train_set = set(split_data['train'])\n",
    "val_set = set(split_data['val'])\n",
    "test_set = set(split_data['test'])\n",
    "assert len(train_set & val_set) == 0, 'Train/Val overlap!'\n",
    "assert len(train_set & test_set) == 0, 'Train/Test overlap!'\n",
    "assert len(val_set & test_set) == 0, 'Val/Test overlap!'\n",
    "print('\\nNo overlap between splits (verified)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Catalog Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = EuclidDataset.load_split(CATALOG_PATH, SPLIT_PATH, 'train')\n",
    "val_df = EuclidDataset.load_split(CATALOG_PATH, SPLIT_PATH, 'val')\n",
    "test_df = EuclidDataset.load_split(CATALOG_PATH, SPLIT_PATH, 'test')\n",
    "\n",
    "print(f'Train: {len(train_df):,}')\n",
    "print(f'Val:   {len(val_df):,}')\n",
    "print(f'Test:  {len(test_df):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Morphology Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = MorphologySchema.default()\n",
    "\n",
    "print(f'Number of output values: {schema.num_outputs}')\n",
    "print(f'\\nQuestions and index slices:')\n",
    "for question, (start, end) in schema.question_slices.items():\n",
    "    answers = schema.questions[question]\n",
    "    print(f'  [{start:2d}:{end:2d}] {question}: {answers}')\n",
    "\n",
    "print(f'\\nFull column list:')\n",
    "for i, col in enumerate(schema.columns):\n",
    "    print(f'  [{i:2d}] {col}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Instantiate Dataset\n",
    "\n",
    "**Note**: This cell requires downloaded images. If images are not yet downloaded,\n",
    "run `python scripts/download_images.py` first (~3.8 GB download).\n",
    "\n",
    "If images are not available, we can still validate the target/mask logic\n",
    "by creating the dataset without transforms and skipping image loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if images are available\n",
    "images_available = IMAGE_DIR.exists() and any(IMAGE_DIR.rglob('*.jpg'))\n",
    "\n",
    "if images_available:\n",
    "    n_images = sum(1 for _ in IMAGE_DIR.rglob('*.jpg'))\n",
    "    print(f'Found {n_images:,} images in {IMAGE_DIR}')\n",
    "else:\n",
    "    print(f'No images found in {IMAGE_DIR}')\n",
    "    print('Run: python scripts/download_images.py')\n",
    "    print('\\nContinuing with target/mask validation only...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets — transforms only if images are available\n",
    "train_transform = get_transforms('train') if images_available else None\n",
    "val_transform = get_transforms('val') if images_available else None\n",
    "\n",
    "train_ds = EuclidDataset(train_df, IMAGE_DIR, schema, train_transform)\n",
    "val_ds = EuclidDataset(val_df, IMAGE_DIR, schema, val_transform)\n",
    "test_ds = EuclidDataset(test_df, IMAGE_DIR, schema, val_transform)\n",
    "\n",
    "print(f'Train dataset: {len(train_ds):,} samples')\n",
    "print(f'Val dataset:   {len(val_ds):,} samples')\n",
    "print(f'Test dataset:  {len(test_ds):,} samples')\n",
    "print(f'Output dim:    {schema.num_outputs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Validate Target & Mask Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the pre-computed target and mask tensors\n",
    "print(f'Target tensor shape: {train_ds.targets.shape}')\n",
    "print(f'Mask tensor shape:   {train_ds.masks.shape}')\n",
    "\n",
    "# Check per-question validity rates\n",
    "print(f'\\nPer-question validity (% of training samples):')\n",
    "for question, (start, end) in schema.question_slices.items():\n",
    "    valid_pct = train_ds.masks[:, start].mean().item() * 100\n",
    "    print(f'  {question:25s}: {valid_pct:5.1f}% valid')\n",
    "\n",
    "# Verify fractions sum to 1 for valid questions\n",
    "print(f'\\nFraction sum check (should be ~1.0 where valid):')\n",
    "for question, (start, end) in schema.question_slices.items():\n",
    "    mask = train_ds.masks[:, start] > 0\n",
    "    if mask.sum() > 0:\n",
    "        sums = train_ds.targets[mask, start:end].sum(dim=1)\n",
    "        print(f'  {question:25s}: mean={sums.mean():.4f}, std={sums.std():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Sample Images\n",
    "\n",
    "Load a few images and display them with their morphology labels.\n",
    "(Skip this section if images are not yet downloaded.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if images_available:\n",
    "    # Use eval transforms so we see the original images (no random augmentation)\n",
    "    viz_ds = EuclidDataset(train_df, IMAGE_DIR, schema, get_transforms('val'))\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 5, figsize=(18, 11))\n",
    "    \n",
    "    # Show 15 random galaxies\n",
    "    rng = np.random.default_rng(42)\n",
    "    indices = rng.choice(len(viz_ds), 15, replace=False)\n",
    "    \n",
    "    # ImageNet denormalize for visualization\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    \n",
    "    for ax, idx in zip(axes.flat, indices):\n",
    "        img, targets, mask = viz_ds[idx]\n",
    "        \n",
    "        # Denormalize\n",
    "        img_vis = img * std + mean\n",
    "        img_vis = img_vis.clamp(0, 1).permute(1, 2, 0).numpy()\n",
    "        \n",
    "        ax.imshow(img_vis)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Label: dominant morphology + key fractions\n",
    "        s_frac = targets[0].item()  # smooth fraction\n",
    "        f_frac = targets[1].item()  # featured fraction\n",
    "        p_frac = targets[2].item()  # problem fraction\n",
    "        ax.set_title(f'S={s_frac:.2f} F={f_frac:.2f} P={p_frac:.2f}', fontsize=9)\n",
    "    \n",
    "    fig.suptitle('Sample Galaxies (S=Smooth, F=Featured, P=Problem)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'data_sample_galaxies.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Skipping visualization — images not downloaded yet.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if images_available:\n",
    "    # Show augmentation effects on a single galaxy\n",
    "    aug_ds = EuclidDataset(train_df, IMAGE_DIR, schema, get_transforms('train'))\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 5, figsize=(18, 7))\n",
    "    fig.suptitle('Same Galaxy Under 10 Random Augmentations', fontsize=14)\n",
    "    \n",
    "    idx = indices[0]  # Pick one galaxy\n",
    "    for ax in axes.flat:\n",
    "        img, _, _ = aug_ds[idx]\n",
    "        img_vis = (img * std + mean).clamp(0, 1).permute(1, 2, 0).numpy()\n",
    "        ax.imshow(img_vis)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'data_augmentation_examples.pdf', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if images_available:\n",
    "    loader = DataLoader(\n",
    "        train_ds, batch_size=32, shuffle=True,\n",
    "        num_workers=0,  # Use 0 for notebook; 4 in scripts\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    batch_img, batch_targets, batch_mask = next(iter(loader))\n",
    "    \n",
    "    print(f'Image batch:  {batch_img.shape}  {batch_img.dtype}')\n",
    "    print(f'Target batch: {batch_targets.shape}  {batch_targets.dtype}')\n",
    "    print(f'Mask batch:   {batch_mask.shape}  {batch_mask.dtype}')\n",
    "    print(f'\\nPixel range: [{batch_img.min():.3f}, {batch_img.max():.3f}]')\n",
    "    print(f'Target range: [{batch_targets.min():.3f}, {batch_targets.max():.3f}]')\n",
    "    print(f'Mask sum per sample (mean): {batch_mask.sum(dim=1).mean():.1f} / {schema.num_outputs}')\n",
    "    \n",
    "    # Simulate masked loss computation\n",
    "    fake_pred = torch.randn_like(batch_targets)\n",
    "    per_output_loss = (fake_pred - batch_targets) ** 2\n",
    "    masked_loss = (per_output_loss * batch_mask).sum() / batch_mask.sum()\n",
    "    print(f'\\nSimulated masked MSE loss: {masked_loss.item():.4f}')\n",
    "    print('DataLoader test PASSED')\n",
    "else:\n",
    "    print('Skipping DataLoader test — images not downloaded yet.')\n",
    "    print('\\nTarget/mask tensors are ready. Once images are downloaded,')\n",
    "    print('the pipeline will be fully operational.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. TTA Transforms Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if images_available:\n",
    "    from PIL import Image as PILImage\n",
    "    \n",
    "    tta_transforms = get_tta_transforms()\n",
    "    print(f'Number of TTA views: {len(tta_transforms)}')\n",
    "    \n",
    "    # Load one raw image\n",
    "    row = train_df.iloc[indices[0]]\n",
    "    tile = row['tile_index']\n",
    "    obj_id = str(row['object_id']).replace('-', 'NEG')\n",
    "    fname = f'{tile}_{obj_id}_gz_arcsinh_vis_y.jpg'\n",
    "    img_path = IMAGE_DIR / str(tile) / fname\n",
    "    raw_img = PILImage.open(img_path).convert('RGB')\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 7, figsize=(21, 3))\n",
    "    tta_labels = ['Original', 'H-Flip', 'V-Flip', '90°', '180°', '270°', 'H-Flip+90°']\n",
    "    \n",
    "    for ax, tfm, label in zip(axes, tta_transforms, tta_labels):\n",
    "        img_t = tfm(raw_img)\n",
    "        img_vis = (img_t * std + mean).clamp(0, 1).permute(1, 2, 0).numpy()\n",
    "        ax.imshow(img_vis)\n",
    "        ax.set_title(label, fontsize=10)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    fig.suptitle('Test-Time Augmentation Views', fontsize=13)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'data_tta_views.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    tta_transforms = get_tta_transforms()\n",
    "    print(f'TTA configured with {len(tta_transforms)} views')\n",
    "    print('Visual preview available after image download.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "### Pipeline Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 60)\n",
    "print('DATA PIPELINE STATUS')\n",
    "print('=' * 60)\n",
    "print(f'\\n  Catalog:        {\"OK\" if CATALOG_PATH.exists() else \"MISSING\"} ({CATALOG_PATH.name})')\n",
    "print(f'  Splits:         {\"OK\" if SPLIT_PATH.exists() else \"MISSING\"} ({SPLIT_PATH.name})')\n",
    "print(f'  Images:         {\"OK\" if images_available else \"PENDING\"} (run download_images.py)')\n",
    "print(f'  Schema:         {schema.num_outputs} outputs across {len(schema.questions)} questions')\n",
    "print(f'  Train samples:  {len(train_ds):,}')\n",
    "print(f'  Val samples:    {len(val_ds):,}')\n",
    "print(f'  Test samples:   {len(test_ds):,}')\n",
    "print(f'  TTA views:      {len(get_tta_transforms())}')\n",
    "print(f'\\n  Ready for training: {\"YES\" if images_available else \"NO (need images)\"}')\n",
    "print('=' * 60)\n",
    "\n",
    "if not images_available:\n",
    "    print('\\nNext step: python scripts/download_images.py')\n",
    "    print('  This downloads ~3.8 GB of VIS+Y galaxy cutouts from Zenodo.')\n",
    "    print('  After download, re-run this notebook to validate the full pipeline.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
