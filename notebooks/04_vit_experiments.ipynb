{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 — ViT Experiments: Transformer & Modern CNN Training\n",
    "\n",
    "**Goal**: Train and evaluate four architectures on Euclid Q1 galaxy morphology,\n",
    "using the same pipeline as the Zoobot baseline (notebook 03).\n",
    "\n",
    "| Model | Architecture | Params | Pre-training | Input size |\n",
    "|-------|-------------|--------|-------------|------------|\n",
    "| ViT-Base/16 | Vanilla ViT | ~86M | ImageNet-21k | 224×224 |\n",
    "| Swin-V2-Base | Hierarchical ViT | ~88M | ImageNet-21k | 256×256 |\n",
    "| DINOv2 ViT-B/14 | SSL Transformer | ~87M | LVD-142M | 224×224 |\n",
    "| ConvNeXt-Base | Modern CNN | ~89M | ImageNet-21k | 224×224 |\n",
    "\n",
    "Each model follows the same two-phase training:\n",
    "1. **Linear probe** (frozen backbone, 5 epochs, lr=1e-3)\n",
    "2. **Full fine-tune** (all layers, 25 epochs, lr=5e-5, cosine + warmup)\n",
    "\n",
    "Results saved per model for comparison in notebook 05.\n",
    "\n",
    "**Runtime**: ~30-40 min per model on A100 (~2-3 hours total for all 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Colab Setup\n",
    "\n",
    "Run this cell **only on Google Colab** — it clones the repo, installs dependencies, and downloads the data. Skip if running locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "IN_COLAB = 'COLAB_GPU' in os.environ or 'google.colab' in str(get_ipython())\n",
    "\n",
    "if IN_COLAB:\n",
    "    # 1. Clone the repo (dev branch has all source code)\n",
    "    REPO_URL = 'https://github.com/Smooth-Cactus0/euclid-q1-vit-morphology.git'\n",
    "    REPO_DIR = '/content/euclid-q1-vit-morphology'\n",
    "\n",
    "    if not os.path.exists(REPO_DIR):\n",
    "        print('Cloning repository (dev branch)...')\n",
    "        !git clone --branch dev {REPO_URL} {REPO_DIR}\n",
    "    os.chdir(REPO_DIR)\n",
    "    print(f'Working directory: {os.getcwd()}')\n",
    "\n",
    "    # 2. Install dependencies\n",
    "    print('\\nInstalling dependencies...')\n",
    "    !pip install -q timm tqdm scipy\n",
    "\n",
    "    # 3. Download catalog\n",
    "    CATALOG_PATH = 'data/raw/morphology_catalogue.parquet'\n",
    "    if not os.path.exists(CATALOG_PATH):\n",
    "        print('\\nDownloading catalog...')\n",
    "        !python scripts/download_catalog.py\n",
    "\n",
    "    # 4. Generate splits\n",
    "    SPLIT_PATH = 'data/processed/split_indices.json'\n",
    "    if not os.path.exists(SPLIT_PATH):\n",
    "        print('\\nGenerating train/val/test splits...')\n",
    "        !python scripts/prepare_splits.py\n",
    "\n",
    "    # 5. Download and extract images (~3.8 GB, takes ~5 min)\n",
    "    IMAGE_DIR = 'data/raw/images'\n",
    "    if not os.path.exists(IMAGE_DIR) or len(os.listdir(IMAGE_DIR)) < 10:\n",
    "        print('\\nDownloading images (~3.8 GB)... This takes ~5 minutes.')\n",
    "        !python scripts/download_images.py\n",
    "    else:\n",
    "        n_tiles = len([d for d in os.listdir(IMAGE_DIR) if os.path.isdir(os.path.join(IMAGE_DIR, d))])\n",
    "        print(f'\\nImages already present: {n_tiles} tiles')\n",
    "\n",
    "    print('\\nColab setup complete!')\n",
    "else:\n",
    "    print('Not running on Colab — skipping setup.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Set project root — Colab runs from repo root, local runs from notebooks/\n",
    "if 'COLAB_GPU' in os.environ or 'google.colab' in str(get_ipython()):\n",
    "    PROJECT_ROOT = Path('/content/euclid-q1-vit-morphology')\n",
    "else:\n",
    "    PROJECT_ROOT = Path('..').resolve()\n",
    "\n",
    "# Ensure project root is on sys.path\n",
    "project_str = str(PROJECT_ROOT)\n",
    "sys.path = [p for p in sys.path if p != project_str]\n",
    "sys.path.insert(0, project_str)\n",
    "\n",
    "print(f'PROJECT_ROOT: {PROJECT_ROOT}')\n",
    "print(f'src/ exists: {(PROJECT_ROOT / \"src\").exists()}')\n",
    "print(f'src/models/architectures.py exists: {(PROJECT_ROOT / \"src\" / \"models\" / \"architectures.py\").exists()}')\n",
    "\n",
    "import json\n",
    "import time\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.data.dataset import EuclidDataset, MorphologySchema\n",
    "from src.data.transforms import get_transforms, get_tta_transforms\n",
    "from src.models.factory import create_model, list_models\n",
    "from src.training.losses import DirichletMultinomialLoss\n",
    "from src.training.trainer import Trainer, TrainConfig\n",
    "from src.evaluation.metrics import (\n",
    "    compute_metrics,\n",
    "    compute_per_question_metrics,\n",
    "    bootstrap_confidence_interval,\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'\\nDevice: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    gpu_mem_gb = props.total_memory / 1e9\n",
    "    print(f'GPU: {torch.cuda.get_device_name()}')\n",
    "    print(f'Memory: {gpu_mem_gb:.1f} GB')\n",
    "\n",
    "print(f'\\nAvailable models: {list_models()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "CATALOG_PATH = PROJECT_ROOT / 'data' / 'raw' / 'morphology_catalogue.parquet'\n",
    "SPLIT_PATH = PROJECT_ROOT / 'data' / 'processed' / 'split_indices.json'\n",
    "IMAGE_DIR = PROJECT_ROOT / 'data' / 'raw' / 'images'\n",
    "CHECKPOINT_DIR = PROJECT_ROOT / 'results' / 'checkpoints'\n",
    "FIGURES_DIR = PROJECT_ROOT / 'results' / 'figures'\n",
    "TABLES_DIR = PROJECT_ROOT / 'results' / 'tables'\n",
    "\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TABLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Models to train (in order)\n",
    "MODELS_TO_TRAIN = ['vit-base', 'swin-v2', 'dinov2', 'convnext']\n",
    "\n",
    "# Model-specific input sizes\n",
    "# Swin-V2-Base was pretrained at 192→256, so we use 256 for best performance.\n",
    "# DINOv2 patch14: 224/14=16 patches — works perfectly.\n",
    "# ViT-Base patch16: 224/16=14 patches — standard.\n",
    "MODEL_INPUT_SIZES = {\n",
    "    'vit-base': 224,\n",
    "    'swin-v2': 256,\n",
    "    'dinov2': 224,\n",
    "    'convnext': 224,\n",
    "}\n",
    "\n",
    "# Training hyperparameters (from configs/base.yaml)\n",
    "LR_PROBE = 1e-3\n",
    "LR_FINETUNE = 5e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "LINEAR_PROBE_EPOCHS = 5\n",
    "FINETUNE_EPOCHS = 25\n",
    "PATIENCE = 5\n",
    "NUM_WORKERS = 2 if IN_COLAB else 4\n",
    "\n",
    "# Base batch size — adjusted per model and GPU\n",
    "BASE_BATCH_SIZE = 32\n",
    "if torch.cuda.is_available():\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    if gpu_mem > 30:       # A100\n",
    "        BASE_BATCH_SIZE = 64\n",
    "    elif gpu_mem > 14:     # V100 / T4\n",
    "        BASE_BATCH_SIZE = 32\n",
    "    else:\n",
    "        BASE_BATCH_SIZE = 16\n",
    "\n",
    "# Swin-V2 at 256×256 uses more memory; reduce batch size if needed\n",
    "MODEL_BATCH_SIZES = {\n",
    "    'vit-base': BASE_BATCH_SIZE,\n",
    "    'swin-v2': max(BASE_BATCH_SIZE // 2, 16),  # 256×256 needs more VRAM\n",
    "    'dinov2': BASE_BATCH_SIZE,\n",
    "    'convnext': BASE_BATCH_SIZE,\n",
    "}\n",
    "\n",
    "# Augmentation config (matching configs/base.yaml)\n",
    "AUGMENTATION_CFG = {\n",
    "    'random_horizontal_flip': True,\n",
    "    'random_vertical_flip': True,\n",
    "    'random_rotation': 360,\n",
    "    'color_jitter': {\n",
    "        'brightness': 0.1,\n",
    "        'contrast': 0.1,\n",
    "        'saturation': 0.0,\n",
    "        'hue': 0.0,\n",
    "    },\n",
    "    'random_resized_crop': {\n",
    "        'scale': [0.85, 1.0],\n",
    "        'ratio': [0.95, 1.05],\n",
    "    },\n",
    "}\n",
    "\n",
    "print('Models to train:', MODELS_TO_TRAIN)\n",
    "print('Batch sizes:', MODEL_BATCH_SIZES)\n",
    "print('Input sizes:', MODEL_INPUT_SIZES)\n",
    "print(f'Num workers: {NUM_WORKERS}')\n",
    "print(f'Training: {LINEAR_PROBE_EPOCHS} probe + {FINETUNE_EPOCHS} fine-tune epochs per model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading\n",
    "\n",
    "Load the catalog splits once. DataLoaders are rebuilt per model (since input sizes differ)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema\n",
    "schema = MorphologySchema.default()\n",
    "print(f'Schema: {schema.num_outputs} outputs across {len(schema.questions)} questions')\n",
    "\n",
    "# Load splits (once — reused across all models)\n",
    "train_df = EuclidDataset.load_split(CATALOG_PATH, SPLIT_PATH, 'train')\n",
    "val_df = EuclidDataset.load_split(CATALOG_PATH, SPLIT_PATH, 'val')\n",
    "test_df = EuclidDataset.load_split(CATALOG_PATH, SPLIT_PATH, 'test')\n",
    "\n",
    "print(f'\\nTrain: {len(train_df):,}')\n",
    "print(f'Val:   {len(val_df):,}')\n",
    "print(f'Test:  {len(test_df):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataloaders(input_size, batch_size):\n",
    "    \"\"\"Build train/val/test DataLoaders for a given input size and batch size.\"\"\"\n",
    "    train_tfm = get_transforms('train', input_size, augmentation_cfg=AUGMENTATION_CFG)\n",
    "    val_tfm = get_transforms('val', input_size)\n",
    "\n",
    "    train_ds = EuclidDataset(train_df, IMAGE_DIR, schema, train_tfm)\n",
    "    val_ds = EuclidDataset(val_df, IMAGE_DIR, schema, val_tfm)\n",
    "    test_ds = EuclidDataset(test_df, IMAGE_DIR, schema, val_tfm)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=NUM_WORKERS, pin_memory=True, drop_last=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=NUM_WORKERS, pin_memory=True,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_ds, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=NUM_WORKERS, pin_memory=True,\n",
    "    )\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "print('DataLoader factory ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict(model, loader, device):\n",
    "    \"\"\"Run inference and collect predictions, targets, masks.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_targets, all_masks = [], [], []\n",
    "    \n",
    "    for images, targets, masks in loader:\n",
    "        images = images.to(device)\n",
    "        logits = model(images)\n",
    "        \n",
    "        # Per-question softmax to get vote fractions\n",
    "        preds = torch.zeros_like(logits)\n",
    "        for q, (start, end) in schema.question_slices.items():\n",
    "            preds[:, start:end] = torch.softmax(logits[:, start:end], dim=1)\n",
    "        \n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(targets.numpy())\n",
    "        all_masks.append(masks.numpy())\n",
    "    \n",
    "    return (\n",
    "        np.concatenate(all_preds),\n",
    "        np.concatenate(all_targets),\n",
    "        np.concatenate(all_masks),\n",
    "    )\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_with_tta(model, dataset_df, image_dir, schema, device,\n",
    "                     input_size=224, batch_size=32, num_workers=2):\n",
    "    \"\"\"Run TTA inference: predict with each transform, then average.\"\"\"\n",
    "    model.eval()\n",
    "    tta_transforms = get_tta_transforms(input_size=input_size)\n",
    "    print(f'  TTA with {len(tta_transforms)} views')\n",
    "    \n",
    "    all_view_preds = []\n",
    "    \n",
    "    for i, tfm in enumerate(tta_transforms):\n",
    "        ds = EuclidDataset(dataset_df, image_dir, schema, tfm)\n",
    "        loader = DataLoader(\n",
    "            ds, batch_size=batch_size, shuffle=False,\n",
    "            num_workers=num_workers, pin_memory=True,\n",
    "        )\n",
    "        \n",
    "        preds_list = []\n",
    "        for images, _, _ in loader:\n",
    "            images = images.to(device)\n",
    "            logits = model(images)\n",
    "            preds = torch.zeros_like(logits)\n",
    "            for q, (start, end) in schema.question_slices.items():\n",
    "                preds[:, start:end] = torch.softmax(logits[:, start:end], dim=1)\n",
    "            preds_list.append(preds.cpu().numpy())\n",
    "        \n",
    "        all_view_preds.append(np.concatenate(preds_list))\n",
    "        print(f'    View {i+1}/{len(tta_transforms)} done')\n",
    "    \n",
    "    # Average across TTA views\n",
    "    tta_preds = np.mean(all_view_preds, axis=0)\n",
    "    \n",
    "    # Get targets and masks\n",
    "    from src.data.transforms import get_transforms as _gt\n",
    "    orig_ds = EuclidDataset(dataset_df, image_dir, schema, _gt('val', input_size))\n",
    "    targets = orig_ds.targets.numpy()\n",
    "    masks = orig_ds.masks.numpy()\n",
    "    \n",
    "    return tta_preds, targets, masks\n",
    "\n",
    "\n",
    "def plot_training_curves(history_df, model_name, best_epoch, best_loss, save_path):\n",
    "    \"\"\"Plot loss, LR, and timing curves for a single model.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    # Loss\n",
    "    ax = axes[0]\n",
    "    ax.plot(history_df['epoch'], history_df['train_loss'], 'b-', label='Train', alpha=0.8)\n",
    "    ax.plot(history_df['epoch'], history_df['val_loss'], 'r-', label='Val', alpha=0.8)\n",
    "    ax.axvline(best_epoch, color='gray', linestyle='--', alpha=0.5, label=f'Best (epoch {best_epoch})')\n",
    "    ax.scatter([best_epoch], [best_loss], color='red', zorder=5, s=50)\n",
    "    probe_epochs = history_df[history_df['phase'] == 'probe']\n",
    "    if len(probe_epochs) > 0:\n",
    "        boundary = probe_epochs['epoch'].max() + 0.5\n",
    "        ax.axvline(boundary, color='green', linestyle=':', alpha=0.5, label='Phase boundary')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Dirichlet-Multinomial Loss')\n",
    "    ax.set_title('Training & Validation Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # LR\n",
    "    ax = axes[1]\n",
    "    ax.plot(history_df['epoch'], history_df['lr'], 'g-', linewidth=2)\n",
    "    if len(probe_epochs) > 0:\n",
    "        ax.axvline(boundary, color='green', linestyle=':', alpha=0.5)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Learning Rate')\n",
    "    ax.set_title('LR Schedule')\n",
    "    ax.set_yscale('log')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Timing\n",
    "    ax = axes[2]\n",
    "    colors = ['steelblue' if p == 'probe' else 'coral' for p in history_df['phase']]\n",
    "    ax.bar(history_df['epoch'], history_df['time_s'], color=colors, alpha=0.7)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Time (s)')\n",
    "    ax.set_title('Epoch Duration')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    fig.suptitle(f'{model_name} Training — Best val_loss: {best_loss:.4f}', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print('Helper functions defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop\n",
    "\n",
    "Train each model sequentially. For each model:\n",
    "1. Create model and DataLoaders (with model-specific input size)\n",
    "2. Two-phase training (linear probe → full fine-tune)\n",
    "3. Plot training curves\n",
    "4. Evaluate on test set (with and without TTA)\n",
    "5. Bootstrap confidence intervals\n",
    "6. Save all results\n",
    "7. Free GPU memory before the next model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def mse_metric(preds, targets, masks):\n",
    "    valid = masks.astype(bool).flatten()\n",
    "    return mean_squared_error(targets.flatten()[valid], preds.flatten()[valid])\n",
    "\n",
    "def r2_metric(preds, targets, masks):\n",
    "    valid = masks.astype(bool).flatten()\n",
    "    return r2_score(targets.flatten()[valid], preds.flatten()[valid])\n",
    "\n",
    "\n",
    "# Store all results for comparison\n",
    "all_results = {}\n",
    "\n",
    "for model_name in MODELS_TO_TRAIN:\n",
    "    print(f'\\n{\"#\" * 70}')\n",
    "    print(f'# MODEL: {model_name}')\n",
    "    print(f'{\"#\" * 70}')\n",
    "    \n",
    "    input_size = MODEL_INPUT_SIZES[model_name]\n",
    "    batch_size = MODEL_BATCH_SIZES[model_name]\n",
    "    \n",
    "    # --- Seed reset for fair comparison ---\n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "    \n",
    "    # --- 1. DataLoaders ---\n",
    "    print(f'\\n  Input size: {input_size}×{input_size}, batch_size: {batch_size}')\n",
    "    train_loader, val_loader, test_loader = make_dataloaders(input_size, batch_size)\n",
    "    print(f'  Train: {len(train_loader)} batches, Val: {len(val_loader)}, Test: {len(test_loader)}')\n",
    "    \n",
    "    # --- 2. Model ---\n",
    "    print(f'\\n  Creating model: {model_name}...')\n",
    "    model = create_model(model_name, num_outputs=schema.num_outputs, pretrained=True)\n",
    "    params = model.count_parameters()\n",
    "    print(f'  Parameters: {params[\"total\"]:,} total, {params[\"trainable\"]:,} trainable')\n",
    "    \n",
    "    # Quick forward-pass sanity check\n",
    "    with torch.no_grad():\n",
    "        dummy = torch.randn(2, 3, input_size, input_size)\n",
    "        out = model(dummy)\n",
    "        print(f'  Forward pass OK: {dummy.shape} → {out.shape}')\n",
    "    \n",
    "    # --- 3. Train ---\n",
    "    criterion = DirichletMultinomialLoss(schema)\n",
    "    train_config = TrainConfig(\n",
    "        lr=LR_FINETUNE,\n",
    "        lr_linear_probe=LR_PROBE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        batch_size=batch_size,\n",
    "        epochs=LINEAR_PROBE_EPOCHS + FINETUNE_EPOCHS,\n",
    "        linear_probe_epochs=LINEAR_PROBE_EPOCHS,\n",
    "        full_finetune_epochs=FINETUNE_EPOCHS,\n",
    "        warmup_fraction=0.05,\n",
    "        patience=PATIENCE,\n",
    "        checkpoint_dir=str(CHECKPOINT_DIR),\n",
    "        seed=SEED,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        config=train_config,\n",
    "        device=device,\n",
    "        model_name=model_name,\n",
    "    )\n",
    "    \n",
    "    print(f'\\n  Starting training...')\n",
    "    t_start = time.time()\n",
    "    summary = trainer.train()\n",
    "    t_total = time.time() - t_start\n",
    "    \n",
    "    best_epoch = summary['best_epoch']\n",
    "    best_loss = summary['best_val_loss']\n",
    "    print(f'\\n  Training time: {t_total/60:.1f} min')\n",
    "    print(f'  Best val_loss: {best_loss:.4f} (epoch {best_epoch})')\n",
    "    \n",
    "    # --- 4. Training curves ---\n",
    "    history = pd.DataFrame(trainer.history)\n",
    "    plot_training_curves(\n",
    "        history, model_name, best_epoch, best_loss,\n",
    "        FIGURES_DIR / f'{model_name}_training_curves.pdf',\n",
    "    )\n",
    "    \n",
    "    # --- 5. Load best checkpoint & evaluate ---\n",
    "    best_ckpt = CHECKPOINT_DIR / f'{model_name}_best.pt'\n",
    "    model.load_state_dict(torch.load(best_ckpt, map_location=device, weights_only=True))\n",
    "    model.to(device)\n",
    "    \n",
    "    # Standard evaluation (no TTA)\n",
    "    print(f'\\n  Evaluating on test set (no TTA)...')\n",
    "    test_preds, test_targets, test_masks = predict(model, test_loader, device)\n",
    "    metrics = compute_metrics(test_preds, test_targets, test_masks, schema)\n",
    "    per_q = compute_per_question_metrics(test_preds, test_targets, test_masks, schema)\n",
    "    \n",
    "    print(f'  MSE={metrics[\"mse\"]:.4f}  MAE={metrics[\"mae\"]:.4f}  '\n",
    "          f'R²={metrics[\"r2\"]:.4f}  Acc={metrics.get(\"accuracy_mean\", float(\"nan\")):.4f}')\n",
    "    \n",
    "    # TTA evaluation\n",
    "    print(f'\\n  Evaluating with TTA...')\n",
    "    tta_preds, tta_targets, tta_masks = predict_with_tta(\n",
    "        model, test_df, IMAGE_DIR, schema, device,\n",
    "        input_size=input_size, batch_size=batch_size, num_workers=NUM_WORKERS,\n",
    "    )\n",
    "    tta_metrics = compute_metrics(tta_preds, tta_targets, tta_masks, schema)\n",
    "    tta_per_q = compute_per_question_metrics(tta_preds, tta_targets, tta_masks, schema)\n",
    "    \n",
    "    print(f'  MSE={tta_metrics[\"mse\"]:.4f}  MAE={tta_metrics[\"mae\"]:.4f}  '\n",
    "          f'R²={tta_metrics[\"r2\"]:.4f}  Acc={tta_metrics.get(\"accuracy_mean\", float(\"nan\")):.4f}')\n",
    "    \n",
    "    # --- 6. Bootstrap CIs ---\n",
    "    print(f'\\n  Computing bootstrap CIs (1000 iterations)...')\n",
    "    mse_point, mse_lo, mse_hi = bootstrap_confidence_interval(\n",
    "        tta_preds, tta_targets, tta_masks, mse_metric, n_iterations=1000,\n",
    "    )\n",
    "    r2_point, r2_lo, r2_hi = bootstrap_confidence_interval(\n",
    "        tta_preds, tta_targets, tta_masks, r2_metric, n_iterations=1000,\n",
    "    )\n",
    "    print(f'  MSE (TTA): {mse_point:.4f}  [{mse_lo:.4f}, {mse_hi:.4f}]')\n",
    "    print(f'  R²  (TTA): {r2_point:.4f}  [{r2_lo:.4f}, {r2_hi:.4f}]')\n",
    "    \n",
    "    # --- 7. Save results ---\n",
    "    model_results = {\n",
    "        'model_name': model_name,\n",
    "        'architecture': model.__class__.__name__,\n",
    "        'pretrained': 'ImageNet-21k' if model_name != 'dinov2' else 'LVD-142M (DINO)',\n",
    "        'input_size': input_size,\n",
    "        'params_total': params['total'],\n",
    "        'params_trainable': params['trainable'],\n",
    "        'training': {\n",
    "            'batch_size': batch_size,\n",
    "            'lr_probe': LR_PROBE,\n",
    "            'lr_finetune': LR_FINETUNE,\n",
    "            'linear_probe_epochs': LINEAR_PROBE_EPOCHS,\n",
    "            'finetune_epochs': FINETUNE_EPOCHS,\n",
    "            'total_epochs_trained': len(trainer.history),\n",
    "            'best_epoch': best_epoch,\n",
    "            'best_val_loss': best_loss,\n",
    "            'training_time_min': t_total / 60,\n",
    "        },\n",
    "        'metrics_no_tta': metrics,\n",
    "        'metrics_tta': tta_metrics,\n",
    "        'per_question_tta': {q: dict(m) for q, m in tta_per_q.items()},\n",
    "        'bootstrap_ci': {\n",
    "            'mse': {'point': mse_point, 'ci_lower': mse_lo, 'ci_upper': mse_hi},\n",
    "            'r2': {'point': r2_point, 'ci_lower': r2_lo, 'ci_upper': r2_hi},\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    # Save JSON\n",
    "    results_path = TABLES_DIR / f'{model_name}_results.json'\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(model_results, f, indent=2, default=float)\n",
    "    print(f'  Results saved: {results_path}')\n",
    "    \n",
    "    # Save predictions\n",
    "    np.savez_compressed(\n",
    "        TABLES_DIR / f'{model_name}_predictions.npz',\n",
    "        predictions=tta_preds,\n",
    "        targets=tta_targets,\n",
    "        masks=tta_masks,\n",
    "    )\n",
    "    \n",
    "    # Save per-question CSV\n",
    "    pq_rows = []\n",
    "    for question, qm in tta_per_q.items():\n",
    "        if qm.get('n_valid', 0) == 0:\n",
    "            continue\n",
    "        pq_rows.append({\n",
    "            'question': question,\n",
    "            'n_valid': qm['n_valid'],\n",
    "            'mse': qm['mse'],\n",
    "            'mae': qm['mae'],\n",
    "            'r2': qm.get('r2', np.nan),\n",
    "            'pearson_r': qm.get('pearson_r', np.nan),\n",
    "            'accuracy': qm.get('accuracy', np.nan),\n",
    "            'f1_weighted': qm.get('f1_weighted', np.nan),\n",
    "        })\n",
    "    pd.DataFrame(pq_rows).to_csv(TABLES_DIR / f'{model_name}_per_question.csv', index=False)\n",
    "    \n",
    "    # Save training history\n",
    "    history.to_csv(TABLES_DIR / f'{model_name}_history.csv', index=False)\n",
    "    \n",
    "    all_results[model_name] = model_results\n",
    "    \n",
    "    # --- 8. Cleanup GPU memory ---\n",
    "    del model, trainer, criterion\n",
    "    del train_loader, val_loader, test_loader\n",
    "    del test_preds, test_targets, test_masks\n",
    "    del tta_preds, tta_targets, tta_masks\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f'\\n  {model_name} complete! GPU memory freed.')\n",
    "\n",
    "print(f'\\n{\"=\" * 70}')\n",
    "print(f'ALL {len(MODELS_TO_TRAIN)} MODELS TRAINED SUCCESSFULLY')\n",
    "print(f'{\"=\" * 70}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Quick Comparison Table\n",
    "\n",
    "Side-by-side comparison of all trained models. The full benchmarking analysis is in notebook 05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build comparison DataFrame\n",
    "comp_rows = []\n",
    "\n",
    "# Include Zoobot baseline if available\n",
    "zoobot_path = TABLES_DIR / 'zoobot_results.json'\n",
    "if zoobot_path.exists():\n",
    "    with open(zoobot_path) as f:\n",
    "        zoobot_results = json.load(f)\n",
    "    comp_rows.append({\n",
    "        'Model': 'Zoobot (EfficientNet-B0)',\n",
    "        'Params (M)': zoobot_results['params_total'] / 1e6,\n",
    "        'MSE': zoobot_results['metrics_tta']['mse'],\n",
    "        'MAE': zoobot_results['metrics_tta']['mae'],\n",
    "        'R²': zoobot_results['metrics_tta']['r2'],\n",
    "        'Pearson r': zoobot_results['metrics_tta'].get('pearson_r', np.nan),\n",
    "        'Accuracy': zoobot_results['metrics_tta'].get('accuracy_mean', np.nan),\n",
    "        'Train time (min)': zoobot_results['training']['training_time_min'],\n",
    "    })\n",
    "\n",
    "# Add all ViT experiment models\n",
    "model_display_names = {\n",
    "    'vit-base': 'ViT-Base/16',\n",
    "    'swin-v2': 'Swin-V2-Base',\n",
    "    'dinov2': 'DINOv2 ViT-B/14',\n",
    "    'convnext': 'ConvNeXt-Base',\n",
    "}\n",
    "\n",
    "for model_name, results in all_results.items():\n",
    "    tta = results['metrics_tta']\n",
    "    comp_rows.append({\n",
    "        'Model': model_display_names.get(model_name, model_name),\n",
    "        'Params (M)': results['params_total'] / 1e6,\n",
    "        'MSE': tta['mse'],\n",
    "        'MAE': tta['mae'],\n",
    "        'R²': tta['r2'],\n",
    "        'Pearson r': tta.get('pearson_r', np.nan),\n",
    "        'Accuracy': tta.get('accuracy_mean', np.nan),\n",
    "        'Train time (min)': results['training']['training_time_min'],\n",
    "    })\n",
    "\n",
    "comp_df = pd.DataFrame(comp_rows)\n",
    "print(comp_df.to_string(index=False, float_format='{:.4f}'.format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart: R² and MSE comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "models = comp_df['Model']\n",
    "x = np.arange(len(models))\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, len(models)))\n",
    "\n",
    "# R²\n",
    "ax = axes[0]\n",
    "bars = ax.barh(x, comp_df['R²'], color=colors, edgecolor='gray', alpha=0.8)\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(models, fontsize=10)\n",
    "ax.set_xlabel('R² (higher is better)')\n",
    "ax.set_title('R² — All Models (with TTA)')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "for bar, val in zip(bars, comp_df['R²']):\n",
    "    ax.text(bar.get_width() + 0.005, bar.get_y() + bar.get_height()/2,\n",
    "            f'{val:.4f}', va='center', fontsize=9)\n",
    "\n",
    "# MSE\n",
    "ax = axes[1]\n",
    "bars = ax.barh(x, comp_df['MSE'], color=colors, edgecolor='gray', alpha=0.8)\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(models, fontsize=10)\n",
    "ax.set_xlabel('MSE (lower is better)')\n",
    "ax.set_title('MSE — All Models (with TTA)')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "for bar, val in zip(bars, comp_df['MSE']):\n",
    "    ax.text(bar.get_width() + 0.0005, bar.get_y() + bar.get_height()/2,\n",
    "            f'{val:.4f}', va='center', fontsize=9)\n",
    "\n",
    "fig.suptitle('Model Comparison — Test Set Metrics (with TTA)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'model_comparison_r2_mse.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Per-Question Comparison\n",
    "\n",
    "Which model is best at each morphology question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load per-question CSVs for all models\n",
    "all_model_names = ['zoobot'] + MODELS_TO_TRAIN\n",
    "all_display_names = {\n",
    "    'zoobot': 'Zoobot',\n",
    "    **model_display_names,\n",
    "}\n",
    "\n",
    "per_q_data = {}\n",
    "for mn in all_model_names:\n",
    "    csv_path = TABLES_DIR / f'{mn}_per_question.csv'\n",
    "    if csv_path.exists():\n",
    "        per_q_data[mn] = pd.read_csv(csv_path)\n",
    "\n",
    "if len(per_q_data) > 1:\n",
    "    questions = per_q_data[list(per_q_data.keys())[0]]['question'].tolist()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "    \n",
    "    x = np.arange(len(questions))\n",
    "    width = 0.8 / len(per_q_data)\n",
    "    model_colors = plt.cm.Set2(np.linspace(0, 1, len(per_q_data)))\n",
    "    \n",
    "    for i, (mn, df) in enumerate(per_q_data.items()):\n",
    "        offset = (i - len(per_q_data)/2 + 0.5) * width\n",
    "        \n",
    "        # R² per question\n",
    "        axes[0].barh(x + offset, df['r2'], height=width,\n",
    "                     label=all_display_names.get(mn, mn),\n",
    "                     color=model_colors[i], alpha=0.8, edgecolor='gray')\n",
    "        \n",
    "        # Accuracy per question\n",
    "        axes[1].barh(x + offset, df['accuracy'], height=width,\n",
    "                     label=all_display_names.get(mn, mn),\n",
    "                     color=model_colors[i], alpha=0.8, edgecolor='gray')\n",
    "    \n",
    "    for ax, metric_name in zip(axes, ['R²', 'Accuracy']):\n",
    "        ax.set_yticks(x)\n",
    "        ax.set_yticklabels(questions, fontsize=10)\n",
    "        ax.set_xlabel(metric_name)\n",
    "        ax.set_title(f'{metric_name} per Morphology Question')\n",
    "        ax.legend(loc='lower right', fontsize=9)\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    fig.suptitle('Per-Question Model Comparison (with TTA)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'per_question_model_comparison.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Not enough models with per-question results for comparison.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Dynamics Comparison\n",
    "\n",
    "Overlay validation loss curves for all models to see convergence speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "line_styles = ['-', '--', '-.', ':', '-']\n",
    "model_colors = plt.cm.tab10(np.linspace(0, 0.5, len(all_model_names)))\n",
    "\n",
    "for i, mn in enumerate(all_model_names):\n",
    "    hist_path = TABLES_DIR / f'{mn}_history.csv'\n",
    "    if not hist_path.exists():\n",
    "        continue\n",
    "    hist = pd.read_csv(hist_path)\n",
    "    ax.plot(hist['epoch'], hist['val_loss'],\n",
    "            linestyle=line_styles[i % len(line_styles)],\n",
    "            color=model_colors[i],\n",
    "            label=all_display_names.get(mn, mn),\n",
    "            linewidth=2, alpha=0.8)\n",
    "\n",
    "ax.axvline(LINEAR_PROBE_EPOCHS + 0.5, color='gray', linestyle=':', alpha=0.4,\n",
    "           label='Probe → Fine-tune')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Validation Loss (DM)')\n",
    "ax.set_title('Validation Loss Convergence — All Models')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'all_models_val_loss.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 70)\n",
    "print('NOTEBOOK 04 — ViT EXPERIMENTS SUMMARY')\n",
    "print('=' * 70)\n",
    "\n",
    "for model_name, results in all_results.items():\n",
    "    tta = results['metrics_tta']\n",
    "    ci = results['bootstrap_ci']\n",
    "    t = results['training']\n",
    "    print(f'''\n",
    "  {model_display_names.get(model_name, model_name)}\n",
    "    Parameters:   {results['params_total']:,}\n",
    "    Input size:   {results['input_size']}×{results['input_size']}\n",
    "    Best epoch:   {t['best_epoch']} / {t['total_epochs_trained']}\n",
    "    Train time:   {t['training_time_min']:.1f} min\n",
    "    MSE (TTA):    {tta['mse']:.4f}  [{ci['mse']['ci_lower']:.4f}, {ci['mse']['ci_upper']:.4f}]\n",
    "    R² (TTA):     {tta['r2']:.4f}  [{ci['r2']['ci_lower']:.4f}, {ci['r2']['ci_upper']:.4f}]\n",
    "    Pearson r:    {tta.get('pearson_r', float('nan')):.4f}\n",
    "    Accuracy:     {tta.get('accuracy_mean', float('nan')):.4f}''')\n",
    "\n",
    "print(f'''\n",
    "{'=' * 70}\n",
    "All results saved in: {TABLES_DIR}/\n",
    "  - <model>_results.json       (full metrics + CIs)\n",
    "  - <model>_predictions.npz    (raw predictions for post-hoc analysis)\n",
    "  - <model>_per_question.csv   (per-question breakdown)\n",
    "  - <model>_history.csv        (training curves)\n",
    "\n",
    "Next: notebooks/05_benchmarking.ipynb\n",
    "  - Aggregate comparison tables (LaTeX-ready)\n",
    "  - Statistical significance tests (paired bootstrap)\n",
    "  - Inference speed benchmarking\n",
    "{'=' * 70}''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}